name: Process Links and Deploy Site

on:
  push:
    branches: [ main ]
    paths: 
      - 'links.md'
      - 'config/**'
      - 'scripts/**'
      - '.github/workflows/**'
  workflow_dispatch: # Allow manual triggering

permissions:
  contents: read
  pages: write
  id-token: write

# Allow only one concurrent deployment, skipping runs queued between the run in-progress and latest queued.
concurrency:
  group: "pages"
  cancel-in-progress: false

jobs:
  process-and-deploy:
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0 # Full history for git log timestamps
    
    - name: Setup Bun
      uses: oven-sh/setup-bun@v1
      with:
        bun-version: latest
    
    - name: Install dependencies
      run: bun install
    
    - name: Download existing metadata from GitHub Pages (non-blocking)
      continue-on-error: true
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        echo "üîç Attempting to download existing metadata from GitHub Pages..."
        
        # Get the GitHub Pages URL (non-blocking)
        pages_url=$(gh api "repos/$GITHUB_REPOSITORY/pages" --jq '.html_url' 2>/dev/null || echo "")
        
        if [ -n "$pages_url" ]; then
          echo "üì° Found GitHub Pages URL: $pages_url"
          
          # Create build_output directory if it doesn't exist
          mkdir -p build_output/data
          
          # Try to download existing data archive (non-blocking)
          data_url="${pages_url}data.tar.gz"
          echo "üì¶ Attempting to download data archive from: $data_url"
          
          if curl -f -s -L "$data_url" -o "build_output/data.tar.gz" 2>/dev/null; then
            echo "‚úÖ Successfully downloaded existing data archive"
            
            # Extract the complete data archive
            if cd build_output && tar -xzf data.tar.gz 2>/dev/null && rm data.tar.gz && cd ..; then
              if [ -d "build_output/data" ]; then
                echo "üìä Extracted data directory with:"
                echo "  - $(ls build_output/data/scraped/ 2>/dev/null | wc -l) scraped articles"
                echo "  - $(ls build_output/data/enriched/ 2>/dev/null | wc -l) enriched articles"
              fi
            fi
          else
            echo "‚ÑπÔ∏è  No existing data archive found (this is normal for first deployment)"
            
            # Try to download just the dataset as fallback (non-blocking)
            dataset_url="${pages_url}dataset.json"
            echo "üì• Attempting to download dataset from: $dataset_url"
            
            if curl -f -s -L "$dataset_url" -o "build_output/existing_dataset.json" 2>/dev/null; then
              echo "‚úÖ Successfully downloaded existing dataset as fallback"
            else
              echo "‚ÑπÔ∏è  No existing dataset found (this is normal for first deployment)"
            fi
          fi
        else
          echo "‚ÑπÔ∏è  GitHub Pages not yet configured or accessible"
        fi
        
        echo "üèÅ Metadata download attempt completed"

    - name: Restore cached data
      uses: actions/cache@v3
      with:
        path: |
          build_output/data/
        key: basted-pocket-data-${{ hashFiles('links.md') }}
        restore-keys: |
          basted-pocket-data-

    - name: Scrape content from URLs
      run: |
        # Only refresh scraped content older than 30 days
        REFRESH_DATE=$(date -d '30 days ago' --iso-8601)
        bun run scripts/scrapeContent.ts --refresh-since $REFRESH_DATE

    - name: Generate static site (without enrichment to save API costs)
      run: bun run scripts/generateSite.ts
    
    - name: Setup Pages
      uses: actions/configure-pages@v4
    
    - name: Upload artifact
      uses: actions/upload-pages-artifact@v3
      with:
        path: './dist'
    
    - name: Deploy to GitHub Pages
      id: deployment
      uses: actions/deploy-pages@v4 